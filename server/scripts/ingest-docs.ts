import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';
// import pdf from 'pdf-parse'; // Avoid index.js side effects
import pdf from 'pdf-parse/lib/pdf-parse.js';
import { RecursiveCharacterTextSplitter } from '@langchain/textsplitters';
import { createClient } from '@supabase/supabase-js';
import dotenv from 'dotenv';
import { AlibabaTongyiEmbeddings } from '../utils/aliyun-embeddings.js';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Load environment variables
dotenv.config({ path: path.resolve(__dirname, '../../.env') });

const SUPABASE_URL = process.env.SUPABASE_URL;
const SUPABASE_KEY = process.env.SUPABASE_SERVICE_KEY || process.env.SUPABASE_SERVICE_ROLE_KEY || process.env.SUPABASE_ANON_KEY;
const DASHSCOPE_API_KEY = process.env.DASHSCOPE_API_KEY;

if (!SUPABASE_URL || !SUPABASE_KEY || !DASHSCOPE_API_KEY) {
  console.error('âŒ Missing environment variables. Please check .env file.');
  process.exit(1);
}

const supabase = createClient(SUPABASE_URL, SUPABASE_KEY);

const embeddings = new AlibabaTongyiEmbeddings({
  apiKey: DASHSCOPE_API_KEY
});

const DOCS_DIR = path.resolve(__dirname, '../../documents/source');

async function processPdf(filePath: string) {
  console.log(`ğŸ“„ Processing file: ${filePath}`);
  const dataBuffer = fs.readFileSync(filePath);
  const data = await pdf(dataBuffer);
  
  // Basic metadata
  const metadata = {
    source: path.basename(filePath),
    page_count: data.numpages,
    info: data.info,
  };

  // Split text
  // ä¼˜åŒ–ç­–ç•¥1ï¼šå¢å¤§ Chunk Size ä»¥ä¿ç•™æ›´å¤šä¸Šä¸‹æ–‡
  const splitter = new RecursiveCharacterTextSplitter({
    chunkSize: 1000, // å¢å¤§åˆ° 1000
    chunkOverlap: 200, // å¢å¤§é‡å éƒ¨åˆ†
  });

  // ä¼˜åŒ–ç­–ç•¥4ï¼šå…ƒæ•°æ®æå– (Metadata Extraction)
  // å°è¯•ä»æ–‡ä»¶åä¸­æå–å¹´ä»½å’Œå¯èƒ½çš„éƒ¨é—¨ä¿¡æ¯
  // ä¾‹å¦‚: "ä¿¡æ¯ä¸æ§åˆ¶å·¥ç¨‹å­¦é™¢2024å¹´æ¨å…å·¥ä½œå®æ–½ç»†åˆ™.pdf"
  const fileName = path.basename(filePath);
  const yearMatch = fileName.match(/20\d{2}/);
  const year = yearMatch ? parseInt(yearMatch[0]) : undefined;
  
  // ç®€å•çš„å…³é”®è¯åŒ¹é…éƒ¨é—¨ (å¯ä»¥æ ¹æ®å®é™…æƒ…å†µæ‰©å±•)
  let department = 'å­¦æ ¡';
  if (fileName.includes('ä¿¡æ¯ä¸æ§åˆ¶') || fileName.includes('ä¿¡æ§')) {
    department = 'ä¿¡æ¯ä¸æ§åˆ¶å·¥ç¨‹å­¦é™¢';
  } else if (fileName.includes('æ•™åŠ¡å¤„')) {
    department = 'æ•™åŠ¡å¤„';
  }

  // æ›´æ–° metadata
  const enrichedMetadata = {
    ...metadata,
    year,
    department,
    // æ·»åŠ ä¸€ä¸ªç”¨äºæ··åˆæ£€ç´¢çš„å…³é”®è¯å­—æ®µï¼Œè™½ç„¶ pgvector ä¹Ÿå¯ä»¥æœ contentï¼Œä½†åˆ†å¼€å¯èƒ½æ›´æ¸…æ™°
    keywords: [year, department].filter(Boolean).join(' ') 
  };

  const docs = await splitter.createDocuments([data.text], [enrichedMetadata]);
  console.log(`âœ‚ï¸  Split into ${docs.length} chunks (Size: 1000, Overlap: 200).`);

  // Generate embeddings and save to Supabase
  for (let i = 0; i < docs.length; i++) {
    const doc = docs[i];
    const embedding = await embeddings.embedQuery(doc.pageContent);

    const { error } = await supabase.from('documents').insert({
      content: doc.pageContent,
      metadata: doc.metadata, // ä½¿ç”¨å¢å¼ºåçš„ metadata
      embedding,
    });

    if (error) {
      console.error('âŒ Error saving document chunk:', error);
    } else {
      process.stdout.write('.'); // Progress indicator
    }
  }
  console.log('\nâœ… File processed successfully.');
}

async function main() {
  if (!fs.existsSync(DOCS_DIR)) {
    console.log(`Creating directory: ${DOCS_DIR}`);
    fs.mkdirSync(DOCS_DIR, { recursive: true });
  }

  const files = fs.readdirSync(DOCS_DIR).filter(file => file.endsWith('.pdf'));

  if (files.length === 0) {
    console.log('âš ï¸  No PDF files found in documents/source. Please add some files to ingest.');
    return;
  }

  console.log(`ğŸ” Found ${files.length} PDF files.`);

  for (const file of files) {
    // Check if file already exists in database to avoid duplicates/re-embedding
    const { count, error } = await supabase
      .from('documents')
      .select('id', { count: 'exact', head: true })
      .filter('metadata->>source', 'eq', file);
    
    if (error) {
      console.error(`âŒ Error checking file status for ${file}:`, error);
      continue;
    }

    if (count && count > 0) {
      console.log(`â© Skipping ${file} (already processed). Use --force to re-process.`);
      continue;
    }

    await processPdf(path.join(DOCS_DIR, file));
  }

  console.log('ğŸ‰ All files processed!');
}

main().catch(console.error);
